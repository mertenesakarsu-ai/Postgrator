import psycopg
from psycopg import sql
import logging
from typing import List, Dict, Any
from services.type_mapper import map_mssql_to_pg_type
import io

logger = logging.getLogger(__name__)

async def get_pg_connection(pg_uri: str):
    """Get PostgreSQL connection"""
    return await psycopg.AsyncConnection.connect(pg_uri)

async def create_schema(conn, schema_name: str):
    """Create schema if not exists"""
    async with conn.cursor() as cursor:
        await cursor.execute(sql.SQL("CREATE SCHEMA IF NOT EXISTS {}").format(
            sql.Identifier(schema_name)
        ))
        await conn.commit()

async def generate_and_apply_ddl(conn, schema_info: Dict[str, Any], target_schema: str) -> str:
    """
    Generate and apply DDL for all tables (without constraints)
    Returns DDL SQL string
    """
    ddl_statements = []
    
    async with conn.cursor() as cursor:
        for table in schema_info['tables']:
            table_name = table['name'].lower()
            
            # Build column definitions
            col_defs = []
            for col in table['columns']:
                col_name = col['name'].lower()
                pg_type = map_mssql_to_pg_type(
                    col['type'],
                    col['max_length'],
                    col['precision'],
                    col['scale']
                )
                
                null_clause = "" if col['is_nullable'] else "NOT NULL"
                
                # Handle IDENTITY columns
                if col['is_identity']:
                    if 'INT' in pg_type.upper():
                        pg_type = pg_type.replace('INTEGER', 'INTEGER GENERATED BY DEFAULT AS IDENTITY')
                        pg_type = pg_type.replace('BIGINT', 'BIGINT GENERATED BY DEFAULT AS IDENTITY')
                
                col_defs.append(f"{col_name} {pg_type} {null_clause}".strip())
            
            # Create table
            create_table = f"""
            CREATE TABLE IF NOT EXISTS {target_schema}.{table_name} (
                {', '.join(col_defs)}
            )
            """
            
            ddl_statements.append(create_table)
            await cursor.execute(create_table)
            logger.info(f"Created table: {target_schema}.{table_name}")
        
        await conn.commit()
    
    return "\n\n".join(ddl_statements)

async def copy_data_to_table(conn, schema_name: str, table_name: str, columns: List[str], rows: List[tuple]):
    """
    Copy data using COPY FROM STDIN for performance
    """
    if not rows:
        return
    
    table_name = table_name.lower()
    col_names = [c.lower() for c in columns]
    
    async with conn.cursor() as cursor:
        # Create CSV-like data in memory
        data_io = io.StringIO()
        for row in rows:
            # Convert values to strings, handle None
            str_row = []
            for val in row:
                if val is None:
                    str_row.append('\\N')
                elif isinstance(val, bytes):
                    # Convert bytes to hex for BYTEA
                    str_row.append('\\\\x' + val.hex())
                elif isinstance(val, bool):
                    str_row.append('t' if val else 'f')
                else:
                    # Escape special characters
                    str_val = str(val).replace('\\', '\\\\').replace('\t', '\\t').replace('\n', '\\n').replace('\r', '\\r')
                    str_row.append(str_val)
            
            data_io.write('\t'.join(str_row) + '\n')
        
        data_io.seek(0)
        
        # Use COPY FROM STDIN
        async with cursor.copy(
            f"COPY {schema_name}.{table_name} ({', '.join(col_names)}) FROM STDIN"
        ) as copy:
            await copy.write(data_io.read())
        
        await conn.commit()

async def truncate_table(conn, schema_name: str, table_name: str):
    """Truncate table with RESTART IDENTITY"""
    table_name = table_name.lower()
    async with conn.cursor() as cursor:
        await cursor.execute(sql.SQL("TRUNCATE TABLE {} RESTART IDENTITY CASCADE").format(
            sql.Identifier(schema_name, table_name)
        ))
        await conn.commit()

async def apply_primary_keys(conn, schema_info: Dict[str, Any], target_schema: str) -> List[str]:
    """
    Apply primary key constraints
    Returns list of SQL statements
    """
    pk_statements = []
    
    async with conn.cursor() as cursor:
        for table in schema_info['tables']:
            if not table.get('primary_key'):
                continue
            
            table_name = table['name'].lower()
            pk_cols = [c.lower() for c in table['primary_key']['columns']]
            
            pk_name = f"pk_{table_name}"
            pk_sql = f"ALTER TABLE {target_schema}.{table_name} ADD CONSTRAINT {pk_name} PRIMARY KEY ({', '.join(pk_cols)})"
            
            try:
                await cursor.execute(pk_sql)
                pk_statements.append(pk_sql)
                logger.info(f"Applied PK on {target_schema}.{table_name}")
            except Exception as e:
                logger.warning(f"Failed to apply PK on {table_name}: {e}")
        
        await conn.commit()
    
    return pk_statements

async def apply_foreign_keys(conn, schema_info: Dict[str, Any], target_schema: str) -> List[str]:
    """
    Apply foreign key constraints
    Returns list of SQL statements
    """
    fk_statements = []
    
    async with conn.cursor() as cursor:
        for table in schema_info['tables']:
            if not table.get('foreign_keys'):
                continue
            
            table_name = table['name'].lower()
            
            for fk in table['foreign_keys']:
                fk_name = f"fk_{table_name}_{fk['column'].lower()}"
                ref_table = fk['ref_table'].lower()
                fk_sql = f"""
                ALTER TABLE {target_schema}.{table_name} 
                ADD CONSTRAINT {fk_name} 
                FOREIGN KEY ({fk['column'].lower()}) 
                REFERENCES {target_schema}.{ref_table}({fk['ref_column'].lower()})
                """
                
                try:
                    await cursor.execute(fk_sql)
                    fk_statements.append(fk_sql)
                    logger.info(f"Applied FK: {fk_name}")
                except Exception as e:
                    logger.warning(f"Failed to apply FK {fk_name}: {e}")
        
        await conn.commit()
    
    return fk_statements

async def apply_indexes(conn, schema_info: Dict[str, Any], target_schema: str) -> List[str]:
    """
    Apply indexes
    Returns list of SQL statements
    """
    idx_statements = []
    
    async with conn.cursor() as cursor:
        for table in schema_info['tables']:
            if not table.get('indexes'):
                continue
            
            table_name = table['name'].lower()
            
            for idx in table['indexes']:
                idx_name = f"idx_{table_name}_{idx['name'].lower()}"
                unique = "UNIQUE" if idx['is_unique'] else ""
                cols = ', '.join([c.lower() for c in idx['columns']])
                
                idx_sql = f"CREATE {unique} INDEX IF NOT EXISTS {idx_name} ON {target_schema}.{table_name} ({cols})"
                
                try:
                    await cursor.execute(idx_sql)
                    idx_statements.append(idx_sql)
                    logger.info(f"Applied index: {idx_name}")
                except Exception as e:
                    logger.warning(f"Failed to apply index {idx_name}: {e}")
        
        await conn.commit()
    
    return idx_statements

async def get_table_row_count(conn, schema_name: str, table_name: str) -> int:
    """Get row count from PostgreSQL table"""
    table_name = table_name.lower()
    async with conn.cursor() as cursor:
        await cursor.execute(sql.SQL("SELECT COUNT(*) FROM {}").format(
            sql.Identifier(schema_name, table_name)
        ))
        result = await cursor.fetchone()
        return result[0] if result else 0

async def fetch_table_data_paginated(conn, schema_name: str, table_name: str, page: int, page_size: int) -> tuple[List[str], List[tuple], int]:
    """
    Fetch paginated data from PostgreSQL table
    Returns: (column_names, rows, total_count)
    """
    table_name = table_name.lower()
    
    async with conn.cursor() as cursor:
        # Get total count
        await cursor.execute(sql.SQL("SELECT COUNT(*) FROM {}").format(
            sql.Identifier(schema_name, table_name)
        ))
        total = (await cursor.fetchone())[0]
        
        # Get column names
        await cursor.execute(sql.SQL(
            "SELECT column_name FROM information_schema.columns WHERE table_schema = %s AND table_name = %s ORDER BY ordinal_position"
        ), (schema_name, table_name))
        columns = [row[0] for row in await cursor.fetchall()]
        
        # Get data
        offset = (page - 1) * page_size
        await cursor.execute(sql.SQL(
            "SELECT * FROM {} LIMIT %s OFFSET %s"
        ).format(sql.Identifier(schema_name, table_name)), (page_size, offset))
        
        rows = await cursor.fetchall()
        
        # Convert bytes to hex string for display
        display_rows = []
        for row in rows:
            display_row = []
            for val in row:
                if isinstance(val, bytes):
                    display_row.append(f"<BYTEA: {len(val)} bytes>")
                else:
                    display_row.append(val)
            display_rows.append(tuple(display_row))
        
        return columns, display_rows, total
